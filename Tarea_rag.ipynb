{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4ca3329",
   "metadata": {},
   "source": [
    "### Configuración y Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc52b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dependencias cargadas\n",
      "✓ PDF seleccionado: OLLAMA.pdf\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Cargar variables de entorno\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "# *** Aqui se cambia el PDF ***\n",
    "PDF_PATH = \"OLLAMA.pdf\"\n",
    "\n",
    "print(\" Dependencias cargadas\")\n",
    "print(f\" PDF seleccionado: {PDF_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8a8246",
   "metadata": {},
   "source": [
    "### Cargar Documento PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a730c086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ PDF cargado: 16971 caracteres\n"
     ]
    }
   ],
   "source": [
    "def upload_pdf(url: str):\n",
    "    try:\n",
    "        loader = PyPDFLoader(url)\n",
    "        loader = loader.lazy_load()\n",
    "\n",
    "        text = \"\"\n",
    "        for page in loader:\n",
    "            text += page.page_content + \"\\n\"\n",
    "\n",
    "        print(f\" PDF cargado: {len(text)} caracteres\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\" Error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "raw_text = upload_pdf(PDF_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc08d7e",
   "metadata": {},
   "source": [
    "### Experimentación: Diferentes Configuraciones de Text Splitter\n",
    "\n",
    "Prueba diferentes valores de `chunk_size` y `chunk_overlap` para ver cómo afectan los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1d3634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunks creados: 39\n"
     ]
    }
   ],
   "source": [
    "# Configuración 1: PEQUEÑO (más chunks, menos contexto)\n",
    "def text_splitter_small(text):\n",
    "    splitter = CharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50,\n",
    "        separator=\"\\n\"\n",
    "    )\n",
    "    return splitter.create_documents([text])\n",
    "\n",
    "# Configuración 2: MEDIANO (equilibrio)\n",
    "def text_splitter_medium(text):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    )\n",
    "    return splitter.create_documents([text])\n",
    "\n",
    "# Configuración 3: GRANDE (menos chunks, más contexto)\n",
    "def text_splitter_large(text):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=200,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    )\n",
    "    return splitter.create_documents([text])\n",
    "\n",
    "# USAR LA CONFIGURACIÓN QUE PREFIERAS\n",
    "chunks = text_splitter_small(raw_text)  # ← Cambia a small, medium o large\n",
    "print(f\"Chunks creados: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7770ba5a",
   "metadata": {},
   "source": [
    "### Experimentación: Diferentes Modelos de Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b9ba90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Modelo de embedding seleccionado\n"
     ]
    }
   ],
   "source": [
    "# Opción 1: nomic-embed-text (768D, rápido)\n",
    "# embedding = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "# Opción 2: mxbai-embed-large (1024D, más preciso) - PREDETERMINADO\n",
    "embedding = OllamaEmbeddings(model=\"mxbai-embed-large:latest\")\n",
    "\n",
    "# Opción 3: all-minilm (384D, ultraligero)\n",
    "# embedding = OllamaEmbeddings(model=\"all-minilm:latest\")\n",
    "\n",
    "print(\"Modelo de embedding seleccionado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808df2db",
   "metadata": {},
   "source": [
    "### Base de Datos Vectorial (Chroma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5251a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Base de datos vectorial inicializada\n"
     ]
    }
   ],
   "source": [
    "def get_vector_store(name_collection: str):\n",
    "    vector_store = Chroma(\n",
    "        collection_name=name_collection,\n",
    "        embedding_function=embedding,\n",
    "        persist_directory=\"./vectorstore\"\n",
    "    )\n",
    "    return vector_store\n",
    "\n",
    "vector_store = get_vector_store(\"langchain\")\n",
    "print(\"Base de datos vectorial inicializada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be8be8a",
   "metadata": {},
   "source": [
    "### Agregar Documentos a la Base de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961f7864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Documentos agregados a la base de datos vectorial\n"
     ]
    }
   ],
   "source": [
    "vector_store.add_documents(chunks)\n",
    "print(\"Documentos agregados a la base de datos vectorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec0cc74",
   "metadata": {},
   "source": [
    "### Función de Recuperación (Retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01944681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Función retrieval lista\n"
     ]
    }
   ],
   "source": [
    "def retrieval(input_user: str, k: int = 4):\n",
    "    \"\"\"Busca k documentos relevantes. Aumenta k para más contexto.\"\"\"\n",
    "    docs = vector_store.similarity_search(input_user, k=k)\n",
    "    return docs\n",
    "\n",
    "print(\"Función retrieval lista\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636cc113",
   "metadata": {},
   "source": [
    "### Experimentación: Diferentes Prompts\n",
    "\n",
    "Cambia el prompt para ver cómo afecta la calidad de las respuestas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8879e5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Prompt seleccionado\n"
     ]
    }
   ],
   "source": [
    "# Prompt 1: SIMPLE (directo)\n",
    "prompt_simple = PromptTemplate.from_template(\"\"\"\n",
    "Contexto: {contexto}\n",
    "Pregunta: {input_user}\n",
    "Respuesta:\"\"\")\n",
    "\n",
    "# Prompt 2: CON INSTRUCCIONES (recomendado) - PREDETERMINADO\n",
    "prompt_instructions = PromptTemplate.from_template(\"\"\"\n",
    "Eres un asistente experto. Responde basándote SOLO en el contexto.\n",
    "Si la información no está disponible, responde: \"No tengo esa información\".\n",
    "\n",
    "Contexto: {contexto}\n",
    "Pregunta: {input_user}\n",
    "Respuesta:\"\"\")\n",
    "\n",
    "# Prompt 3: CON FUENTES (detalladp)\n",
    "prompt_with_sources = PromptTemplate.from_template(\"\"\"\n",
    "Eres un asistente experto. Responde basándote en el contexto.\n",
    "Al final, indica de dónde obtuviste la información.\n",
    "\n",
    "Contexto: {contexto}\n",
    "Pregunta: {input_user}\n",
    "\n",
    "Respuesta (incluye atribución de fuentes):\n",
    "\"\"\")\n",
    "\n",
    "# USAR EL PROMPT QUE PREFIERAS\n",
    "prompt = prompt_instructions  # ← Cambia a simple, instructions o with_sources\n",
    "print(\"Prompt seleccionado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b985ea1",
   "metadata": {},
   "source": [
    "### Experimentación: Diferentes Modelos LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e626d86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Función response lista\n"
     ]
    }
   ],
   "source": [
    "def response(input_user: str, contexto: str, model_name: str = \"gemini-2.0-flash\", temperature: float = 0.7):\n",
    "    \"\"\"\n",
    "    Genera respuesta usando LLM con streaming.\n",
    "    \n",
    "    Modelos disponibles:\n",
    "    - gemini-2.0-flash (rápido, recomendado)\n",
    "    - gemini-1.5-pro (más potente)\n",
    "    - gemini-1.5-flash (alternativa)\n",
    "    \"\"\"\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        api_key=api_key,\n",
    "        model=model_name,\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "    for chunk in llm.stream(prompt.format(contexto=contexto, input_user=input_user)):\n",
    "        yield chunk.content\n",
    "\n",
    "print(\"Función response lista\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b825bf3",
   "metadata": {},
   "source": [
    "### Loop Interactivo Mejorado\n",
    "\n",
    "Prueba tu sistema RAG. Escribe `salir` para terminar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28ae7340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant: Ollama esOllama es una herramienta de código abierto, escrita en GO, que permite ejecutar Grandes Modelos una herramienta de código abierto, escrita en GO, que permite ejecutar Grandes Modelos de Lenguajes (LLMs) de forma sencilla y local. Al ejecutar de Lenguajes (LLMs) de forma sencilla y local. Al ejecutar los modelos localmente, se mantiene la propiedad plena de los datos y se evitan posibles riesgos de filtraciones de información sensible. También ayuda a reducir la latencia y la los modelos localmente, se mantiene la propiedad plena de los datos y se evitan posibles riesgos de filtraciones de información sensible. También ayuda a reducir la latencia y la dependencia.\n",
      "\n",
      " dependencia.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        input_user = input(\"\\nHuman: \").strip()\n",
    "        \n",
    "        if not input_user:\n",
    "            continue\n",
    "        \n",
    "        if input_user.lower() in {\"salir\", \"exit\", \"quit\"}:\n",
    "            break\n",
    "        \n",
    "        docs = retrieval(input_user, k=4)\n",
    "        \n",
    "        if docs:\n",
    "            print(\"\\nAssistant: \", end=\"\", flush=True)\n",
    "            for chunk in response(input_user, contexto=docs):\n",
    "                print(chunk, end=\"\", flush=True)\n",
    "            print(\"\\n\")\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d76028",
   "metadata": {},
   "source": [
    "### Análisis: Comparar Configuraciones\n",
    "\n",
    "Ejecuta esta celda para probar rápidamente diferentes configuraciones y compararlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "522da01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparación de Búsquedas:\n",
      "                                 Pregunta  Documentos encontrados  Tiempo (s)  Chars del primer doc\n",
      "¿Cuál es el tema principal del documento?                       3       1.001                   478\n",
      "         ¿Qué información importante hay?                       3       0.722                   495\n",
      "                    ¿Puedes resumir esto?                       3       0.803                   498\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Preguntas de prueba\n",
    "test_questions = [\n",
    "    \"¿Cuál es el tema principal del documento?\",\n",
    "    \"¿Qué información importante hay?\",\n",
    "    \"¿Puedes resumir esto?\"\n",
    "]\n",
    "\n",
    "# Probar diferentes configuraciones\n",
    "results = []\n",
    "\n",
    "for question in test_questions:\n",
    "    start_time = time.time()\n",
    "    docs = retrieval(question, k=3)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    results.append({\n",
    "        \"Pregunta\": question[:50],\n",
    "        \"Documentos encontrados\": len(docs),\n",
    "        \"Tiempo (s)\": round(elapsed, 3),\n",
    "        \"Chars del primer doc\": len(docs[0].page_content) if docs else 0\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\nComparación de Búsquedas:\")\n",
    "print(df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
